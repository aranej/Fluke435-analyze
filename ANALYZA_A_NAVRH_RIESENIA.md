# AnalÃ½za a nÃ¡vrh rieÅ¡enia: Fluke 435 Data Processor

## ğŸ“‹ Executive Summary

Po dÃ´kladnej analÃ½ze Fluke 435 exportovanÃ½ch dÃ¡t a Å¡pecifikÃ¡cie poÅ¾iadaviek odporÃºÄam **modulÃ¡rny prÃ­stup s tromi hlavnÃ½mi komponentmi**:

1. **Data Preprocessor** - ÄŒistenie a normalizÃ¡cia dÃ¡t
2. **Core Processor** - VÃ½poÄty a validÃ¡cie
3. **Report Generator** - XLSX + PNG vÃ½stupy

**KÄ¾ÃºÄovÃ© zistenia:**
- âœ… Fuzzy matching hlaviÄiek funguje vÃ½borne
- âš ï¸ Pandas vyÅ¾aduje Å¡peciÃ¡lne nastavenia pre decimal/thousands
- âš ï¸ SÃºbor obsahuje 2413 stÄºpcov (1588 harmonickÃ½ch = 66% dÃ¡t)
- âœ… Chunking nie je potrebnÃ½ pre <10M riadkov ak pouÅ¾ijeme selective loading

---

## ğŸ” 1. AnalÃ½za existujÃºcich dÃ¡t

### 1.1 Å truktÃºra sÃºboru

```
SÃºbor: 2025-10-25_BD16.txt
VeÄ¾kosÅ¥: 17.4 MB
Riadkov: 1,440 (24 hodÃ­n Ã— 1 minÃºta)
StÄºpcov: 2,413
KÃ³dovanie: CP1250 (Windows Central European)
```

**Rozdelenie stÄºpcov:**
| KategÃ³ria | PoÄet | Podiel | DÃ´leÅ¾itosÅ¥ |
|-----------|-------|--------|------------|
| VÃ½kon (P/S/Q/PF) | 672 | 27.9% | â­â­â­ KritickÃ© |
| PrÃºd | 836 | 34.7% | â­â­ VysokÃ¡ |
| HarmonickÃ© 2-50 | 588 | 24.4% | â­ NÃ­zka (optional) |
| NapÃ¤tie | 248 | 10.3% | â­â­ StrednÃ¡ |
| THD | 33 | 1.4% | â­ NÃ­zka |
| OstatnÃ© | 36 | 1.5% | â­ NÃ­zka |

**Memory footprint:**
```
VÅ¡etky stÄºpce:  ~27 MB (pre 1k riadkov) â†’ 27 GB (pre 1M riadkov) âŒ
Core stÄºpce:     ~0.2 MB (pre 1k riadkov) â†’ 200 MB (pre 1M riadkov) âœ…
```

### 1.2 IdentifikovanÃ© problÃ©my vo formÃ¡te

```python
ProblÃ©m                          VÃ½skyt    PrÃ­klad              Fix
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ChÃ½bajÃºca nula pred Äiarkou   100%      ",870"             â†’ "0,870"
2. NegatÃ­vna bez nuly            100%      "-,123"            â†’ "-0,123"
3. PrÃ¡zdne hodnoty (tab-tab)     100%      "...\t\t..."       â†’ "...\t0,0\t..."
4. Pandas thousands separator    N/A       "83616,805" â†’ 83.6 â†’ thousands=''
```

### 1.3 OverenÃ½ mapping kÄ¾ÃºÄovÃ½ch stÄºpcov

| Parameter | Index | PÃ´vodnÃ½ nÃ¡zov | PrÃ­klad hodnoty |
|-----------|-------|---------------|-----------------|
| DÃ¡tum | 0 | "DÃ¡tum" | 21.10.2025 |
| ÄŒas | 1 | "ÄŒas" | 16:01:00.000 |
| P_total | 123 | "ÄŒinnÃ½ vÃ½kon Celkom Priem" | 83616,805 W |
| S_total | 135 | "KlasickÃ½ VA full Celkom Priem" | 97103,383 VA |
| Q_total | 147 | "KlasickÃ½ VAR Celkom Priem" | -51249,008 VAR |
| PF_total | 159 | "KlasickÃ½ PF Celkom Priem" | ,870 |
| U_L1N | 3 | "NapÃ¤tie L1N Priem" | 228,511 V |
| U_L2N | 6 | "NapÃ¤tie L2N Priem" | 230,108 V |
| U_L3N | 9 | "NapÃ¤tie L3N Priem" | 231,403 V |
| Freq | 100 | "Frekvencia Priem" | 49,999 Hz |

---

## ğŸ—ï¸ 2. NÃ¡vrh architektÃºry

### 2.1 PrÃ­stup 1: MonolitickÃ½ (Single-Pass)

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw TXT    â”‚ (CP1250, 2413 cols)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocess         â”‚
â”‚  - Fix encoding     â”‚
â”‚  - Fix ,/- values   â”‚
â”‚  - Clean TXT export â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Selective Load     â”‚
â”‚  - Core ~20 cols    â”‚
â”‚  - Optional: phases â”‚
â”‚  - Skip harmonics   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compute & Validate â”‚
â”‚  - Energy calc      â”‚
â”‚  - Cross-checks     â”‚
â”‚  - Quality metrics  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Export             â”‚
â”‚  - XLSX (5 sheets)  â”‚
â”‚  - PNG (2 plots)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VÃ½hody:**
- âœ… JednoduchÃ½ na implementÃ¡ciu
- âœ… RÃ½chly pre sÃºbory <100k riadkov
- âœ… Jeden priechod dÃ¡tami

**NevÃ½hody:**
- âŒ ZlÃ¡ Å¡kÃ¡lovateÄ¾nosÅ¥ pre 10M riadkov
- âŒ VysokÃ¡ pamÃ¤Å¥ovÃ¡ nÃ¡roÄnosÅ¥

### 2.2 PrÃ­stup 2: Chunked Processing (OdporÃºÄanÃ©)

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw TXT    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocess Stream    â”‚ â† Read line-by-line
â”‚  - Fix on-the-fly     â”‚   Write to clean.txt
â”‚  - Regex substitution â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunked Reader       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk 1 (20k)   â”‚  â”‚ â† Load selective columns
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Chunk 2 (20k)   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Chunk 3 (20k)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Aggregate Results    â”‚
â”‚  - Concatenate chunks â”‚
â”‚  - Sort by timestamp  â”‚
â”‚  - Compute metrics    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Export               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VÃ½hody:**
- âœ… Å kÃ¡luje na 10M+ riadkov
- âœ… KonÅ¡tantnÃ¡ pamÃ¤Å¥ (~200 MB)
- âœ… Preprocessing on-the-fly

**NevÃ½hody:**
- âŒ KomplexnejÅ¡Ã­ kÃ³d
- âŒ O nieÄo pomalÅ¡Ã­ (ale stÃ¡le <1 min pre 1M riadkov)

### 2.3 PrÃ­stup 3: Hybrid (NajlepÅ¡Ã­ kompromis)

```python
if file_size < 100_MB or row_count < 100_000:
    use_single_pass()  # RÃ½chle
else:
    use_chunked()      # Å kÃ¡lovateÄ¾nÃ©
```

**VÃ½hody:**
- âœ… OptimÃ¡lny vÃ½kon pre malÃ© sÃºbory
- âœ… Å kÃ¡luje pre veÄ¾kÃ© sÃºbory
- âœ… AutomatickÃ¡ detekcia

---

## ğŸš€ 3. ImplementaÄnÃ© odporÃºÄania

### 3.1 Preprocessing - Regex Substitutions

**ProblÃ©m:** Pandas nesprÃ¡vne parsuje hodnoty

**RieÅ¡enie:** Preprocess sÃºbor pred naÄÃ­tanÃ­m

```python
import re

def preprocess_line(line: str) -> str:
    """Fix common formatting issues"""

    # 1. Fix  \-  â†’  -
    line = line.replace(' \\- ', '-')
    line = line.replace('\\-', '-')

    # 2. Fix  -,XXX  â†’  -0,XXX  (negative missing zero)
    line = re.sub(r'(^|\t)-,', r'\1-0,', line)

    # 3. Fix  ,XXX  â†’  0,XXX  (positive missing zero)
    line = re.sub(r'(^|\t),', r'\10,', line)

    # 4. Fix empty values  \t\t  â†’  \t0,0\t
    line = re.sub(r'\t\t', '\t0,0\t', line)

    return line

def preprocess_file(input_path: str, output_path: str):
    """Create clean UTF-8 copy with fixes applied"""

    with open(input_path, 'r', encoding='cp1250', errors='replace') as fin:
        with open(output_path, 'w', encoding='utf-8') as fout:

            for line in fin:
                clean_line = preprocess_line(line)
                fout.write(clean_line)
```

**VÃ½hoda:** JednorÃ¡zovÃ© spustenie, potom pracujeÅ¡ s ÄistÃ½m sÃºborom

### 3.2 Fuzzy Matching hlaviÄiek

```python
import unicodedata
import re
from typing import Optional, List

def remove_diacritics(text: str) -> str:
    """Remove diacritics from text"""
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join([c for c in nfkd if not unicodedata.combining(c)])

def normalize_header(text: str) -> str:
    """Normalize header for fuzzy matching"""
    text = remove_diacritics(text)
    text = text.lower()
    text = re.sub(r'[^a-z0-9]+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    # Unify phase notation
    text = text.replace('l1 n', 'l1n')
    text = text.replace('l2 n', 'l2n')
    text = text.replace('l3 n', 'l3n')
    return text

def find_column(columns: List[str], keywords: List[str],
                prefer: List[str] = ['priem', 'avg']) -> Optional[int]:
    """
    Find column by fuzzy matching keywords

    Args:
        columns: List of column names
        keywords: Required keywords (all must match)
        prefer: Preferred aggregation (try in order)

    Returns:
        Column index or None if not found
    """

    candidates = []

    for i, col in enumerate(columns):
        norm = normalize_header(col)

        # All keywords must be present
        if all(kw in norm for kw in keywords):

            # Score by preference
            score = 0
            for j, pref in enumerate(prefer):
                if pref in norm:
                    score = len(prefer) - j  # Higher = better
                    break

            candidates.append((score, i, col))

    if not candidates:
        return None

    # Sort by score (highest first), then by index (lowest first)
    candidates.sort(key=lambda x: (-x[0], x[1]))

    return candidates[0][1]  # Return index

# Usage
columns = load_header('2025-10-25_BD16.txt')

P_total_idx = find_column(columns, ['cinny', 'vykon', 'celkom'])
S_total_idx = find_column(columns, ['va', 'full', 'celkom'])
Q_total_idx = find_column(columns, ['var', 'celkom'])
PF_total_idx = find_column(columns, ['pf', 'celkom'])

# Per-phase
P_L1N_idx = find_column(columns, ['cinny', 'vykon', 'l1n'])
P_L2N_idx = find_column(columns, ['cinny', 'vykon', 'l2n'])
P_L3N_idx = find_column(columns, ['cinny', 'vykon', 'l3n'])
```

### 3.3 Pandas Loading (SprÃ¡vne nastavenia)

```python
import pandas as pd

def load_data_correct(filepath: str,
                      use_cols: List[int],
                      chunksize: Optional[int] = None) -> pd.DataFrame:
    """
    Load data with correct settings for Fluke format

    CRITICAL SETTINGS:
    - decimal=','        # Decimal separator is COMMA
    - thousands=''       # NO thousands separator (not comma!)
    - encoding='utf-8'   # After preprocessing
    """

    if chunksize is None:
        # Single-pass
        df = pd.read_csv(
            filepath,
            sep='\t',
            encoding='utf-8',
            decimal=',',
            thousands='',  # â† CRITICAL!
            usecols=use_cols,
            on_bad_lines='skip',
            low_memory=False
        )
        return df

    else:
        # Chunked
        chunks = []

        reader = pd.read_csv(
            filepath,
            sep='\t',
            encoding='utf-8',
            decimal=',',
            thousands='',
            usecols=use_cols,
            on_bad_lines='skip',
            chunksize=chunksize,
            low_memory=False
        )

        for chunk in reader:
            chunks.append(chunk)

        return pd.concat(chunks, ignore_index=True)
```

### 3.4 Energy Calculation

```python
import numpy as np

def calculate_energy(df: pd.DataFrame,
                     P_col: str = 'P_total') -> dict:
    """
    Calculate energy with adaptive Î”t

    Returns:
        {
            'E_kWh': float,           # Total energy
            'dt_mode': float,         # Dominant Î”t in seconds
            'dt_histogram': dict,     # Top 3 intervals
            'mixed_sampling': bool    # Flag if multiple Î”t values
        }
    """

    # Compute Î”t
    df['dt'] = df['timestamp'].diff().dt.total_seconds()

    # Find dominant Î”t (mode)
    dt_counts = df['dt'].value_counts()
    dt_mode = dt_counts.index[0]

    # Check for mixed sampling
    top3 = dt_counts.head(3)
    mixed = (top3.iloc[0] / len(df) < 0.95)  # <95% dominant

    # Calculate energy
    dt_h = dt_mode / 3600
    E_kWh = df[P_col].sum() * dt_h / 1000

    return {
        'E_kWh': E_kWh,
        'dt_mode': dt_mode,
        'dt_histogram': dict(top3),
        'mixed_sampling': mixed
    }
```

### 3.5 Cross-Validation

```python
def validate_power_balance(df: pd.DataFrame,
                          P_cols: List[str],
                          S_cols: List[str],
                          Q_cols: List[str]) -> dict:
    """
    Cross-validate power measurements

    Returns metrics with percentiles
    """

    # Sum of phases
    df['P_sum'] = df[P_cols].sum(axis=1)
    df['S_sum'] = df[S_cols].sum(axis=1)
    df['Q_sum'] = df[Q_cols].sum(axis=1) if Q_cols else np.nan

    # Relative errors
    df['P_rel_err'] = np.abs(df['P_sum'] - df['P_total']) / np.abs(df['P_total'] + 1e-6)
    df['S_rel_err'] = np.abs(df['S_sum'] - df['S_total']) / np.abs(df['S_total'] + 1e-6)

    # Vectorova kontrola: SÂ² = PÂ² + QÂ² (len kde Q existuje)
    if 'Q_total' in df.columns:
        df['S_squared_calc'] = np.sqrt(df['P_total']**2 + df['Q_total']**2)
        df['S_vec_err'] = np.abs(df['S_total'] - df['S_squared_calc']) / df['S_total']

    # PF vypoÄÃ­tanÃ½
    df['PF_calc'] = np.clip(df['P_total'] / (df['S_total'] + 1e-6), -1, 1)
    df['PF_diff'] = np.abs(df['PF_total'] - df['PF_calc'])

    return {
        'P_rel_err_mean': df['P_rel_err'].mean(),
        'P_rel_err_p95': df['P_rel_err'].quantile(0.95),
        'S_rel_err_mean': df['S_rel_err'].mean(),
        'S_rel_err_p95': df['S_rel_err'].quantile(0.95),
        'PF_diff_mean': df['PF_diff'].mean(),
        'PF_diff_p95': df['PF_diff'].quantile(0.95),
        'S_vec_err_p95': df.get('S_vec_err', pd.Series([np.nan])).quantile(0.95)
    }
```

---

## ğŸ“Š 4. Output Specification

### 4.1 XLSX Sheets

```python
# Sheet 1: summary
{
    'Measurement_Start': '2025-10-21 16:01:00',
    'Measurement_End': '2025-10-22 16:00:00',
    'Duration_hours': 23.98,
    'Sampling_Interval_s': 60,
    'Total_Samples': 1440,
    'Bad_Lines_Skipped': 0,

    'E_total_kWh': 5.44,
    'E_phase_sum_kWh': 5.42,
    'Delta_E_percent': 0.37,

    'PF_mean': 0.881,
    'PF_diff_mean': 0.003,
    'PF_diff_p95': 0.008,

    'S_vec_err_p95': 0.15,

    'Freq_mean_Hz': 50.01,
    'Freq_min_Hz': 49.93,
    'Freq_max_Hz': 50.05,

    'Voltage_Imbalance_p95_percent': 1.2,

    'Status': 'PASS'  # or 'INFO' or 'ALERT'
}

# Sheet 2: validation
{
    'P_phases_vs_total_mean_rel': 0.002,
    'P_phases_vs_total_p95': 0.005,
    'S_phases_vs_total_mean_rel': 0.003,
    ...
}

# Sheet 3: timeseries_power
columns = ['timestamp', 'P_total', 'S_total', 'Q_total', 'PF_total', 'PF_calc',
           'P_L1N', 'P_L2N', 'P_L3N', 'S_L1N', 'S_L2N', 'S_L3N',
           'U_L1N', 'U_L2N', 'U_L3N', 'F']

# Sheet 4: data_quality
{
    'dt_interval_s': [60, 120, 59],
    'count': [1439, 5, 2],
    'percent': [99.9, 0.3, 0.1]
}

# Sheet 5: mapping_log
{
    'Target_Column': ['P_total', 'S_total', ...],
    'Source_Column': ['ÄŒinnÃ½ vÃ½kon Celkom Priem', 'KlasickÃ½ VA full...', ...],
    'Index': [123, 135, ...]
}
```

### 4.2 PNG Plots

```python
import matplotlib.pyplot as plt

# Plot 1: Power timeseries
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df['timestamp'], df['P_total'] / 1000, label='P (kW)', linewidth=1)
ax.plot(df['timestamp'], df['S_total'] / 1000, label='S (kVA)', linewidth=1, alpha=0.7)
ax.set_xlabel('Time')
ax.set_ylabel('Power [kW / kVA]')
ax.legend()
ax.grid(True, alpha=0.3)
plt.savefig('timeseries_power.png', dpi=150, bbox_inches='tight')

# Plot 2: Power Factor
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df['timestamp'], df['PF_total'], label='PF measured', linewidth=1)
ax.plot(df['timestamp'], df['PF_calc'], label='PF calculated', linewidth=1, linestyle='--')
ax.set_xlabel('Time')
ax.set_ylabel('Power Factor')
ax.set_ylim([0, 1])
ax.legend()
ax.grid(True, alpha=0.3)
plt.savefig('timeseries_pf.png', dpi=150, bbox_inches='tight')
```

---

## âš ï¸ 5. KritickÃ© vÃ½zvy a rieÅ¡enia

### 5.1 HarmonickÃ© vlny (Memory Bomb)

**ProblÃ©m:**
- 1588 stÄºpcov (66% sÃºboru!)
- Pre 1M riadkov = 12.7 GB RAM

**RieÅ¡enia:**
1. **IgnorovaÅ¥ pri core processing** âœ… OdporÃºÄanÃ©
2. **Optional flag `--include-harmonics`** pre detailnÃº analÃ½zu
3. **Separate processing** - spracuj harmonickÃ© osobitne ak potrebnÃ©

```python
# Core processing
core_cols = [0, 1, 3, 6, 9, 99, 100, 123, 135, 147, 159]  # ~15 cols

# Ak uÅ¾Ã­vateÄ¾ chce harmonickÃ©
if args.include_harmonics:
    harmonic_cols = range(186, 774)  # 2nd-50th harmonics
    df_harmonics = load_data(filepath, use_cols=harmonic_cols)
    # Spracuj osobitne
```

### 5.2 Pandas Thousands Separator

**ProblÃ©m:**
```python
# NesprÃ¡vne
pd.read_csv(..., decimal=',')  # Pandas si myslÃ­ Å¾e ',' je thousands!
# "83616,805" â†’ 83.6 âŒ

# SprÃ¡vne
pd.read_csv(..., decimal=',', thousands='')  # Å½iadny thousands separator
# "83616,805" â†’ 83616.805 âœ…
```

**Overenie:**
```python
# Test na prvÃ½ch riadkoch
df_test = pd.read_csv(..., nrows=5)
assert df_test['P_total'].iloc[0] > 1000, "P_total is too small! Check thousands separator"
```

### 5.3 Fuzzy Matching - Ambiguity

**ProblÃ©m:** Viac kandidÃ¡tov pre jeden parameter

**RieÅ¡enie:**
```python
# Preferuj:
# 1. "priem" > "avg" > "min" > "max"
# 2. KratÅ¡Ã­ nÃ¡zov (menej slov)
# 3. NiÅ¾Å¡Ã­ index (skÃ´r v sÃºbore)

candidates.sort(key=lambda x: (
    -x[0],  # Score (higher = better)
    len(x[2].split()),  # Word count (lower = better)
    x[1]  # Index (lower = better)
))
```

### 5.4 Mixed Sampling Rate

**ProblÃ©m:** NekonzistentnÃ© Î”t (napr. 60s, potom 120s)

**RieÅ¡enie:**
```python
if mixed_sampling:
    logger.warning(f"Mixed sampling detected! Top intervals: {dt_hist}")
    # PouÅ¾ij dominantnÃ½ interval pre energy calc
    # Ale flag v reporte
```

---

## ğŸ¯ 6. ImplementaÄnÃ© priority

### Priority 1: MVP (Minimum Viable Product)

**CieÄ¾:** FungovaÅ¥ pre 90% use cases

```python
âœ… Preprocess (fix ,/- values)
âœ… Fuzzy match (P/S/PF/U/F total)
âœ… Load core columns only
âœ… Energy calculation
âœ… Basic validation (Î£P vs P_total)
âœ… XLSX export (summary + timeseries)
âœ… PNG plots (power + PF)
```

**ÄŒas: 2-3 dni**

### Priority 2: Extended Features

```python
âœ… Per-phase power (P/S/Q L1N/L2N/L3N)
âœ… Voltage imbalance
âœ… All validation metrics
âœ… Data quality report
âœ… Acceptance criteria thresholds
âœ… Detailed logging
```

**ÄŒas: +2 dni**

### Priority 3: Advanced

```python
âœ… THD analysis
âœ… Harmonics (optional flag)
âœ… Event detection (sags/swells)
âœ… Chunked processing for 10M+ rows
âœ… Progress bar
âœ… Config file support
```

**ÄŒas: +3 dni**

---

## ğŸ’¡ 7. OdporÃºÄania

### 7.1 KÃ³d Å¡truktÃºra

```
fluke_processor/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ config.py               # Configuration
â”œâ”€â”€ preprocessor.py         # Data cleaning
â”œâ”€â”€ column_mapper.py        # Fuzzy matching
â”œâ”€â”€ data_loader.py          # Pandas loading
â”œâ”€â”€ calculator.py           # Energy & validations
â”œâ”€â”€ validator.py            # Cross-checks
â”œâ”€â”€ exporter.py             # XLSX & PNG generation
â”œâ”€â”€ logger.py               # Logging setup
â””â”€â”€ utils.py                # Helper functions

tests/
â”œâ”€â”€ test_preprocessor.py
â”œâ”€â”€ test_column_mapper.py
â”œâ”€â”€ test_calculator.py
â””â”€â”€ fixtures/
    â””â”€â”€ sample_fluke.txt    # Small test file

docs/
â””â”€â”€ USER_GUIDE.md

examples/
â””â”€â”€ example_usage.py
```

### 7.2 CLI Interface

```bash
# Basic usage
python -m fluke_processor input.txt

# With options
python -m fluke_processor input.txt \
    --output-dir ./results \
    --include-harmonics \
    --chunk-size 50000 \
    --verbose

# Config file
python -m fluke_processor input.txt --config config.yaml
```

### 7.3 Testing Strategy

```python
# Test files
1. small.txt        (100 rows)    - unit tests
2. medium.txt       (10k rows)    - integration tests
3. large.txt        (1M rows)     - performance tests
4. corrupted.txt    (bad format)  - robustness tests
5. mixed_lang.txt   (SK/CZ/EN)    - fuzzy match tests
```

---

## ğŸ“ˆ 8. Performance Estimates

### 8.1 Processing Time

| Riadkov | StÄºpce | Approach | Time | RAM |
|---------|--------|----------|------|-----|
| 1k | Core (~20) | Single | <1s | 10 MB |
| 10k | Core | Single | <3s | 50 MB |
| 100k | Core | Single | 15s | 200 MB |
| 1M | Core | Chunked | 90s | 300 MB |
| 10M | Core | Chunked | 15min | 500 MB |
| 1M | All (2413) | Chunked | 5min | 20 GB âŒ |

### 8.2 Bottlenecks

```
1. Preprocessing (regex):  20% Äasu
2. Pandas loading:         40% Äasu
3. Calculations:           30% Äasu
4. XLSX export:            10% Äasu
```

**OptimalizÃ¡cie:**
- PouÅ¾iÅ¥ `pyarrow` engine namiesto `python` (2Ã— rÃ½chlejÅ¡Ã­)
- Vectorized operations (NumPy)
- Multiprocessing pre chunky (ak potrebnÃ©)

---

## âœ… 9. ZÃ¡ver a Next Steps

### OdporÃºÄanÃ½ prÃ­stup:

**1. Hybrid Architecture** (Adaptive)
   - Auto-detect file size
   - Single-pass pre <100k riadkov
   - Chunked pre >100k riadkov

**2. Modular Design**
   - KaÅ¾dÃ½ modul testovateÄ¾nÃ½ samostatne
   - JasnÃ© API medzi modulmi
   - KonfigurovateÄ¾nÃ© cez CLI/config

**3. Implementation Phases**
   - Week 1: MVP (core functionality)
   - Week 2: Extended features
   - Week 3: Advanced + optimization

### ChceÅ¡, aby som:

A) **Vytvoril prototyp** (funkÄnÃ½ kÃ³d s MVP features)?
B) **PokraÄoval v analÃ½ze** (hlbÅ¡ie testy, edge cases)?
C) **Pripravil test data** (vytvorenie syntetickÃ½ch sÃºborov)?
D) **NieÄo inÃ©**?

---

**Autor:** Claude Code Analysis
**DÃ¡tum:** 2025-11-12
**Verzia:** 1.0
